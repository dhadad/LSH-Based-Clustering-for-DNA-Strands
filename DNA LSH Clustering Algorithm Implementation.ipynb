{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1424cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c66772",
   "metadata": {},
   "source": [
    "Note: the 'Reading Data', 'Construct Setup', 'Accuracy Calculation' are the same as in the 'Clustering-algorithm-single-core' implementation given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d465220",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00488388",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_cl = []       # the whole input\n",
    "with open(r'evyat.txt') as file:\n",
    "    for line in file:\n",
    "        reads_cl.append(line.strip())\n",
    "cnt = 0\n",
    "reads = []          # representatives\n",
    "for i in range(0, len(reads_cl)):\n",
    "    if reads_cl[i] != \"\":\n",
    "        if reads_cl[i][0] == \"*\":\n",
    "            cnt += 1\n",
    "            rep = reads_cl[i - 1]\n",
    "            reads.append(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427d7c3",
   "metadata": {},
   "source": [
    "# Construct Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cdb7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_reps = [] # C_reps = [(Read, Cluster rep of the cluster to which the read belongs to)]\n",
    "C_dict = {} # C_dict = {Cluster rep: All the Reads that belong to that cluster}\n",
    "rep = reads_cl[0]\n",
    "for i in range(1, len(reads_cl)):\n",
    "    if reads_cl[i] != \"\":\n",
    "        if reads_cl[i][0] == \"*\":\n",
    "            if len(C_reps) > 0:\n",
    "                # the last sequence is to be placed in a different cluster\n",
    "                C_dict[rep].pop()\n",
    "                C_reps.pop()\n",
    "            rep = reads_cl[i - 1]\n",
    "            C_dict[rep] = []\n",
    "        else:\n",
    "            C_dict[rep].append(reads_cl[i])\n",
    "            C_reps.append((reads_cl[i], rep))\n",
    "C_reps.sort(key=lambda x: x[0])\n",
    "\n",
    "reads_err = [0] * (len(C_reps))\n",
    "for i in range(0, len(C_reps)):\n",
    "    reads_err[i] = C_reps[i][0]\n",
    "random.shuffle(reads_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2aa24",
   "metadata": {},
   "source": [
    "# Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69248fa4",
   "metadata": {},
   "source": [
    "Will check if the cluster we calculated is a subset of a true cluster. If so, using parameter 'gamma', will determine if enough items of the original cluster resides in ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb35ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_in_C(read, C_reps):\n",
    "    lower = 0\n",
    "    upper = len(C_reps) - 1\n",
    "    while lower <= upper:\n",
    "        mid = lower + int((upper - lower) / 2)\n",
    "        if read == (C_reps[mid][0]):\n",
    "            return C_reps[mid][1]\n",
    "        if read > (C_reps[mid][0]):\n",
    "            lower = mid + 1\n",
    "        else:\n",
    "            upper = mid - 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def comp_clstrs(alg_clstr, org_clstr, gamma, reads_err):\n",
    "    num_exist = 0\n",
    "    if len(alg_clstr) > len(org_clstr):\n",
    "        return 0\n",
    "    for i in range(0, len(alg_clstr)):\n",
    "        flg_exist = 0\n",
    "        for j in range(0, len(org_clstr)):\n",
    "            if reads_err[alg_clstr[i]] == org_clstr[j]:\n",
    "                flg_exist = 1\n",
    "                num_exist += 1\n",
    "                break\n",
    "        if flg_exist == 0:\n",
    "            return 0\n",
    "    if num_exist < gamma * len(org_clstr):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def calc_acrcy(clustering, C_dict, C_reps, gamma, reads_err):\n",
    "    acrcy = 0\n",
    "    for i in range(0, len(clustering)):\n",
    "        if len(clustering[i]) >= 1:\n",
    "            acrcy += comp_clstrs(clustering[i],\n",
    "                                 C_dict[rep_in_C(reads_err[clustering[i][0]], C_reps)], gamma, reads_err)\n",
    "    return acrcy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524322ed",
   "metadata": {},
   "source": [
    "# Naive Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455efca",
   "metadata": {},
   "source": [
    "Simple clustering based on the strand's prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b0460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_clstring(all_reads, n=14):\n",
    "    \"\"\"\n",
    "    Attempt to split the sequences to clusters via a trivial method: use the first n characters\n",
    "    in a sequence as an index, then decide two sequences match based on this prefix being equal\n",
    "    or not.\n",
    "    :param all_reads: the whole input, in the form of an array of strings\n",
    "    :param n: integer, the prefix length to be looked at as a key for the clustering\n",
    "    :return C_til, dict of clusters. In the form of C_til[rep] = [reads assigned to the cluster]\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "    prefix_to_ind = {}\n",
    "    for i in range(len(all_reads)):\n",
    "        if all_reads[i][:n] in prefix_to_ind:\n",
    "            prefix_to_ind[all_reads[i][:n]].append(i)\n",
    "        else:\n",
    "            prefix_to_ind[all_reads[i][:n]] = [i]\n",
    "\n",
    "    C_til = {i: [i] for i in range(len(all_reads))}\n",
    "    for indexes in prefix_to_ind.values():\n",
    "        C_til[indexes[0]] = indexes\n",
    "\n",
    "    print(\"time for naive approach: {}\".format(time.time() - time_start))\n",
    "    return C_til"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0535f",
   "metadata": {},
   "source": [
    "# LSH Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b97c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968e1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgram_val(sub_seq):\n",
    "    \"\"\"\n",
    "    Calculate the value of a Q-gram\n",
    "    :param sub_seq: sub string of the original sequence, of length q\n",
    "    :return: integer, representing the value of the Q-gram\n",
    "    \"\"\"\n",
    "    vals = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "    tot = 0\n",
    "    for pos in range(len(sub_seq)):\n",
    "        tot += (4 ** pos) * vals[sub_seq[pos]]\n",
    "    return tot\n",
    "\n",
    "\n",
    "def seq_numset(seq, q):\n",
    "    \"\"\"\n",
    "    Convert a sequence into a set of numbers\n",
    "    :param seq: an original line from the input file\n",
    "    :param q: length of the divided sub-sequences (Q-grams)\n",
    "    :return: array of integers, each one is the value of a Q-gram\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    for i in range(len(seq) - q + 1):\n",
    "        arr.append(qgram_val(seq[i:i + q]))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def mh_sig(numset, perm):\n",
    "    \"\"\"\n",
    "    Obtain a MH signature for a sequence, based on given representation of it as a number set\n",
    "    the given permutation and the definition for MH signature in the article\n",
    "    :param numset: array of integers, each one is a Q-gram value (so its length is the\n",
    "        original sequence's length minus q)\n",
    "    :param perm: array, permutation of {0,..., 4**q}\n",
    "    :return: MH signature in the form of a single integer\n",
    "    \"\"\"\n",
    "    return min([perm[num] for num in numset])\n",
    "\n",
    "\n",
    "def lsh_sig(numset, perms):\n",
    "    \"\"\"\n",
    "    Obtain a LSH signature for a sequence, converted to its representation as a set of numbers\n",
    "    :param numset: array of integers, each one is a Q-gram value (so its length is the\n",
    "        original sequence's length minus q)\n",
    "    :param perms: array of arrays, each: permutation of {0,..., 4**q}\n",
    "    :return: an array of length equal to the nubmer of permutations given. each element is the\n",
    "        MH signature of the sequence calculated with the permutation with the suitable index.\n",
    "    \"\"\"\n",
    "    lsh = []\n",
    "    for perm in perms:\n",
    "        lsh.append(mh_sig(numset, perm))\n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74b55d",
   "metadata": {},
   "source": [
    "## Algorithm Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f46ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numsets(all_reads, q):\n",
    "    \"\"\"\n",
    "    Generate the numbers sets for all the sequences\n",
    "    :param all_reads: array of all the input sequences\n",
    "    :param q: length of the divided sub-sequences (Q-grams)\n",
    "    :return: a dictionary, mapping a number set for each sequence in the input,\n",
    "        while the key is the index of the sequence in all_reads\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "    numsets = {}\n",
    "    for i in range(len(all_reads)):\n",
    "        numsets[i] = seq_numset(all_reads[i], q)\n",
    "    print(\"time to create number set for each sequence: {}\".format(time.time() - time_start))\n",
    "    return numsets\n",
    "\n",
    "\n",
    "def _lsh_sigs(numsets, m, top):\n",
    "    \"\"\"\n",
    "    Calculate the LSH signature of all the sequnces in the input\n",
    "    :param numsets: array of arrays, each one is a set of number representing a sequence\n",
    "    :param m: size of the LSH signature\n",
    "    :param top: the largest possible number in the sets\n",
    "    :return: array of LSH signatures (each is an array itself)\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "    # generate m permutations\n",
    "    perms = [np.random.permutation(top) for _ in range(m)]\n",
    "    # LSH signature tuple (size m, instead of k, as the original paper suggests) for each sequence\n",
    "    lsh_sigs = [lsh_sig(numsets[i], perms) for i in range(len(numsets))]\n",
    "    print(\"time to create LSH signatures for each sequence: {}\".format(time.time() - time_start))\n",
    "    return lsh_sigs\n",
    "\n",
    "\n",
    "def _add_pair(elem_1, elem_2, C_til):\n",
    "    \"\"\"\n",
    "    Insert a pair of two sequences indices into C_til. In case both didn't appeared yet, we'll\n",
    "    treat the first one as the center of the possible cluster.\n",
    "    :param elem_1, elem_2: the indices of two sequences in all_reads.\n",
    "    :param C_til: array of clusters. In the form of C_til[rep] = [reads assigned to the cluster]\n",
    "    \"\"\"\n",
    "    if len(C_til[elem_2]) > 1 >= len(C_til[elem_1]):\n",
    "        C_til[elem_2].extend(C_til[elem_1])\n",
    "        C_til[elem_1] = []\n",
    "    else:\n",
    "        C_til[elem_1].extend(C_til[elem_2])\n",
    "        C_til[elem_2] = []\n",
    "\n",
    "\n",
    "def lsh_clstering(all_reads, q, k, m, L):\n",
    "    \"\"\"\n",
    "    Run the full clustering algorithm: create the number sets for each sequence, then generate a LSH\n",
    "    signature for each, and finally iterate L times looking for matching pairs, to be inserted to the\n",
    "    same cluster.\n",
    "    :param all_reads: array of strings, each: a DNA sequence from the input\n",
    "    :param m: size of the LSH signature\n",
    "    :param q: length of the divided sub-sequences (Q-grams)\n",
    "    :param k: number of MH signatures in a LSH signature\n",
    "    :param L: number of iterations of the algorithm\n",
    "    :return C_til, dict of clusters. In the form of C_til[rep] = [reads assigned to the cluster]\n",
    "    \"\"\"\n",
    "    numsets = _numsets(all_reads, q)\n",
    "    lsh_sigs = _lsh_sigs(numsets, m, 4 ** q)\n",
    "    C_til = {i: [i] for i in range(len(all_reads))}\n",
    "    for itr in range(L):\n",
    "        time_start = time.time()\n",
    "        pairs = set()\n",
    "        sigs = []\n",
    "        buckets = {}\n",
    "        # first, choose random k elements of the LSH signature\n",
    "        # then, by giving a weight to each one, their sum will act as the new signature\n",
    "        indexes = random.sample(range(m), k)\n",
    "        for lsh in lsh_sigs:\n",
    "            sig = sum(lsh[indexes[i]] * ((4 ** q) ** i) for i in range(k))\n",
    "            sigs.append(sig)\n",
    "\n",
    "        # buckets[sig] = [indexes (from all_reads) of (hopefully) similar sequences]\n",
    "        for i in range(len(all_reads)):\n",
    "            if sigs[i] in buckets:\n",
    "                buckets[sigs[i]].append(i)\n",
    "            else:\n",
    "                buckets[sigs[i]] = [i]\n",
    "\n",
    "        # from each bucket we'll keep pairs. the first element will be announced as center\n",
    "        for elems in buckets.values():\n",
    "            if len(elems) <= 1:\n",
    "                continue\n",
    "            for elem in elems[1:]:\n",
    "                pairs.add((elems[0], elem))\n",
    "\n",
    "        # pairs = sorted(list(pairs))\n",
    "        for pair in pairs:\n",
    "            _add_pair(pair[0], pair[1], C_til)\n",
    "\n",
    "        print(\"time for iteration {} in the algorithm: {}\".format(itr + 1, time.time() - time_start))\n",
    "\n",
    "    return C_til"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40698af2",
   "metadata": {},
   "source": [
    "# Run the script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b9984",
   "metadata": {},
   "source": [
    "Parameters:<br />\n",
    "**q** is the length of a Q-gram, to which a sequence will be divided to. According to supplemantry fig. 3 in the article's supporting information: for k >= 6, the probabilty of sequences from DIFFERENT clusters to be classified as such (decided based on having Jaccard similiarty smaller than 0.05). For k <= 8, we'll have a sufficient Jaccard similiarity for two sequences from the SAME cluster.<br />\n",
    "**k** is the number of MH signatures in the tuple of a LSH signature. Far from k = 3 will result with large amount of false positives \\ false negatives.<br />\n",
    "**m** is not part of the original algorithm desription but more of an implementation addition: instead of generating new **k** permutations in each iteration, in order to calculate the LSH signature, we'll use LSH signature of length **m** for each sequence, and then choose in each iteration **k** items out of it.<br />\n",
    "**L** number of iterations. The goal is to pair sequences if Jaccard similarity > 0.5. It is the same as the expectation for two MH signatures to be equal. Thus, for **k** MH signatures, the probabilty is 0.5^k, and the geometric expectation: 1/0.5^k = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aa6b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'lsh' # choose \"naive\" for using the naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "864e4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to create number set for each sequence: 11.83100938796997\n",
      "time to create LSH signatures for each sequence: 34.13584280014038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adar\\AppData\\Local\\Temp/ipykernel_13144/340047100.py:73: RuntimeWarning: overflow encountered in long_scalars\n",
      "  sig = sum(lsh[indexes[i]] * ((4 ** q) ** i) for i in range(k))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for iteration 1 in the algorithm: 1.600045919418335\n",
      "time for iteration 2 in the algorithm: 1.0302081108093262\n",
      "time for iteration 3 in the algorithm: 0.7931225299835205\n",
      "time for iteration 4 in the algorithm: 1.5124871730804443\n",
      "time for iteration 5 in the algorithm: 2.3609108924865723\n",
      "time for iteration 6 in the algorithm: 0.9857909679412842\n",
      "time for iteration 7 in the algorithm: 0.7979569435119629\n",
      "time for iteration 8 in the algorithm: 1.5150244235992432\n",
      "time for iteration 9 in the algorithm: 1.6554627418518066\n",
      "time for iteration 10 in the algorithm: 1.2950093746185303\n",
      "time for iteration 11 in the algorithm: 0.800710916519165\n",
      "time for iteration 12 in the algorithm: 1.0861270427703857\n",
      "time for iteration 13 in the algorithm: 1.6457953453063965\n",
      "time for iteration 14 in the algorithm: 1.6968536376953125\n",
      "time for iteration 15 in the algorithm: 0.8124735355377197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adar\\AppData\\Local\\Temp/ipykernel_13144/340047100.py:73: RuntimeWarning: overflow encountered in longlong_scalars\n",
      "  sig = sum(lsh[indexes[i]] * ((4 ** q) ** i) for i in range(k))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for iteration 16 in the algorithm: 0.970320463180542\n",
      "time for iteration 17 in the algorithm: 1.451859712600708\n",
      "time for iteration 18 in the algorithm: 1.9267783164978027\n",
      "time for iteration 19 in the algorithm: 0.8830983638763428\n",
      "time for iteration 20 in the algorithm: 1.087623119354248\n",
      "time for iteration 21 in the algorithm: 1.1750316619873047\n",
      "time for iteration 22 in the algorithm: 2.147597312927246\n",
      "time for iteration 23 in the algorithm: 1.1182560920715332\n",
      "time for iteration 24 in the algorithm: 0.9624249935150146\n",
      "time for iteration 25 in the algorithm: 0.877375602722168\n",
      "time for iteration 26 in the algorithm: 1.8650708198547363\n",
      "time for iteration 27 in the algorithm: 1.5211782455444336\n",
      "time for iteration 28 in the algorithm: 0.9636256694793701\n",
      "time for iteration 29 in the algorithm: 0.785668134689331\n",
      "time for iteration 30 in the algorithm: 1.4541449546813965\n",
      "time for iteration 31 in the algorithm: 1.7941248416900635\n",
      "time for iteration 32 in the algorithm: 1.2778148651123047\n"
     ]
    }
   ],
   "source": [
    "if approach == 'naive':\n",
    "    C_til = naive_clstring(reads_err)\n",
    "else:\n",
    "    C_til = lsh_clstering(all_reads=reads_err, q=7, k=5, m=50, L=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6825ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.984 0.978 0.964 0.904 0.73 0.274 0.206\n"
     ]
    }
   ],
   "source": [
    "acrcy1 = calc_acrcy(C_til, C_dict, C_reps, 0.6, reads_err) / len(reads)\n",
    "acrcy2 = calc_acrcy(C_til, C_dict, C_reps, 0.7, reads_err) / len(reads)\n",
    "acrcy3 = calc_acrcy(C_til, C_dict, C_reps, 0.8, reads_err) / len(reads)\n",
    "acrcy4 = calc_acrcy(C_til, C_dict, C_reps, 0.9, reads_err) / len(reads)\n",
    "acrcy5 = calc_acrcy(C_til, C_dict, C_reps, 0.95, reads_err) / len(reads)\n",
    "acrcy6 = calc_acrcy(C_til, C_dict, C_reps, 0.99, reads_err) / len(reads)\n",
    "acrcy7 = calc_acrcy(C_til, C_dict, C_reps, 1, reads_err) / len(reads)\n",
    "print(\"Accuracy:\", acrcy1, acrcy2, acrcy3, acrcy4, acrcy5, acrcy6, acrcy7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
